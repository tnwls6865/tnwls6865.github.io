---
layout: post
title:  "Learning What Not to Segment: A New Perspective on Few-Shot Segmentation ì •ë¦¬"
date:   2022-04-11 10:32:36 +0530
categories: paper
---

ì €ë²ˆì£¼ì— ë‚ ì”¨ê°€ ì¢‹ë‹¤ê³  í–‡ëŠ”ë° ì§€ê¸ˆì€ ë”ì›Œì„œ ì£½ì„ ì§€ê²½ ğŸ˜µâ€ğŸ’«ğŸ˜µâ€ğŸ’«



![img1](\assets\post\post23\img1.png)

ê¸°ì¡´ì— ë‹¤ë¥¸ ë°©ë²•ë“¤ì´ meta learningì„ í†µí•´ generalizationì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ë ¤ê³  ë…¸ë ¥í•¨ í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë°©ë²•ë“¤ì€ seen classì— ëŒ€í•´ biasê°€ ì¡´ì¬í•˜ê³  new classì— ëŒ€í•´ recognitionì˜ ì„±ëŠ¥ì„ ë–¨ì–´íŠ¸ë¦¼ íŠ¹íˆ baseë°ì´í„°ì™€ ìœ ì‚¬í•œ ë²”ì£¼ì— ëŒ€í•œ hard query sampleì— ëŒ€í•´ì„œëŠ”  ì¼ë°˜í™” ì„±ëŠ¥ì´ ë¶•ê´´ë¨ 

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” additional branchë¥¼ ì¶”ê°€ë¡œ ë‘ì–´ì„œ base classë¥¼ í•™ìŠµí•˜ê³  ì´ë¥¼ í™œìš©í•¨ 

ê·¸ë¦¬ê³  queryì™€ supportê°„ì˜ ì°¨ì´ë¥¼ í™œìš©í•´ì„œ adjustment factorë¡œ í™œìš©í•¨ 

* base learner
  ì¶”ê°€ì  branchë¡œ base classesì— ëŒ€í•œ regionì„ ì˜ˆì¸¡ 
  $$f_b^q = F_{conv}(\varepsilon(x^q)) \in R^{c\times H\times W}$$
  $$P_b = softmax(D_b(f^q_b)) \in R^{(1+N_b)\times H\times W}$$ : $$D_b$$ - decoder network (enlarge the spatial scale), $$N_b$$ - number of base categories
  episodic learningì´ ì•„ë‹Œ standard supervised learning 
  $$L_{base} = {1\over n_{bs}} \sum_{i=1}^{n_{bs}}CE(p_{b;i}, m^q_{b;i})$$ : $$P_b$$ - prediction , $$m_b^q$$: ground truth 
  **ê¸°ì¡´ ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì“°ë©´ ë„ˆë¬´ ê¹Šê³  infernece ì†ë„ ì˜¤ë˜ ê±¸ë¦¬ê¸°ë•Œë¬¸ì— meta branchì—ì„œ ê°™ì€ backboneì„ ì–¼ë¦¬ëŠ” ê±¸ ì‹œë„í•´ë´£ì§€ë§Œ standard segmentationì„±ëŠ¥ì´ ë–¨ì–´ì§ ê·¸ë˜ì„œ two training strategyë¥¼ í™œìš© **

* meta learner

  support set : $$S = x^s, m^s$$ query image: $$x^q$$ 

  annotation maskì—ì„œ ê°™ì€ ì¹´í…Œê³ ë¦¬ì¸ê±°ì— ëŒ€í•´ segmentí•˜ëŠ” ê²ƒì´ ëª©í‘œ
  $$f^s_m = F_{1\times1}(\varepsilon(x^s)) \in R^{c\times h\times w}$$
  $$f^q_m = F_{1\times1}(\varepsilon(x^q)) \in R^{c\times h\times w}$$
  crucial class-related cuesë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ì„œ masked average pooling(MAP)ë¥¼ í™œìš© $$(f^s_m, m_s)$$ 
  $$v_s = F_{pool}(f_m^s \odot I(m^s)) \in R^c$$ : $$I$$ - reshape function $$R^{H\times W} -> R^{c\times h\times w}$$
  $$p_m = softmax(D_m(F_{guidance(v_s, f_m^q)})) \in R^{2\times H\times W}$$ : $$F_{guidance}$$-supprot branchì—ì„œ query branchë¡œ segmentation cuesë¥¼ ì œê³µí•˜ëŠ” annotation information ì—­í•  expand & concatenate operation 
  $$L_meta = {1\over n_e} \sum_{i=1}^{n_e}BCE(p_{m;i}, m^q_i)$$ : $$n_e$$ - number of training episodes

* ensemble module 
  ![img2](\assets\post\post23\img2.png)
  meta learnerëŠ” support setì˜ ì˜í–¥ì„ ë°›ê¸° ë•Œë¬¸ì— query-supportì´ë¯¸ì§€ ê°„ì˜ ì°¨ì´ì— ëŒ€í•œ í‰ê°€ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬  coarse predictionì— ëŒ€í•´ì„œ ì¡°ì •í•˜ëŠ” ëª¨ë“ˆì„ ì œì•ˆ 
  background regionì— ê´€ë ¨ëœ ì˜ì—­ì„ ì–»ê¸° ìœ„í•´ base learnerì—ì„œ foreground probabilityë¥¼ í†µí•©í•¨
  $$ p^f_b = \sum_{i=1}^{N_b} p_b^i$$
  fixed backbone ìœ¼ë¡œ ë¶€í„° ì¶”ì¶œí•œ low-level feature $$f^s_{low}, f_{low}^q \in R^{C_1\times H_1\times W_1}$$  ì— ëŒ€í•´ gram matricesë¥¼ ì¶”ì¶œ
  $$A_s = F_{reshape}(f^s_{low}) \in R^{C_1 \times N}$$
  $$ G^s = A_s A_s^\top \in R^{C_1\times C_1}$$
  frobenius normì„ í†µí•´ adjustment processë¥¼ ê°€ì´ë“œí•˜ê¸° ìœ„í•œ ì „ì²´ì ì¸ indicator $$\psi$$ ë¥¼ ì–»ìŒ
  $$ \psi = ||G^s - G^q||_F$$
  coarse resultsì— $$\psi$$ë¥¼ í†µí•©í•¨
  $$p_0^f =F_{ensemble}(F_\psi(p_m^0), p_b^t)$$
  $$p_f = p_f^0 \oplus F_{\psi}(p_m^1)$$
  $$F_\psi, F_{ensemble} $$:  1x1 conv,  $$\oplus$$ : concat
  $$L_{final} = {1 \over n_e} \sum_{i=1}^{n_e}BCE(p_i^q, m_i^q)$$

  $$L = L_{final} + \lambda L_{meta} $$