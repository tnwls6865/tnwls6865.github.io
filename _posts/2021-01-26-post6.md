---
layout: post
title:  "Everybody Dance Now 정리"
date:   2021-01-26 16:012:36 +0530
categories: paper
---

맥북 주문한게 왔는데 불량이다 ㅠ 🤬

![img1](\assets\post\post6\img1.png)  
사람이 춤추는 동영상에서 새로운 target video에 transfer하는 논문  
video-to-video tranlation 느낌으로 보면 될거 같음  
source에서 부터 pose를 추출하고 target을 생성하기 위해 pose-to-appearancce mapping을 학습함  
시간적으로 일관된 비디오 결과를 위해 두개의 연속된 프레임을 예측함  
그리고 target video에 대해서 realistic을 위해 face 와 full image로 pipeline을 분리함  

#### Introduction

본 논문에서 simple하지만 효과적인 approach를 제안함 **"Do as I Do" video retargeting**  
두 개의 video가 주어지면 하나는 source 하나는 target  
source에서 target으로 motion을 transfer하는 논문 

frame-by-frame 방법으로 두 비디오 간의 motion을 transfer하기 위해서 두 개개인에 대해 이미지 간의 mapping을 학습해야함  
그래서 pix2pix(image-to-image translation)와 비슷하지만 본 논문은 pair한 이미지를 사용하지 않고, pair한 이미지가 있어도 두 object가 각 유니크한 body shape과 motion을 갖고 있어서 다름 

keypoint기반 포즈는 motion signatures를 유지하기 때문에 pose detector를 사용해서 pose를 추출함  
그리고 나서 pose stick figure이랑 target person 이미지들 간의 image-to-image를 translation model을 학습함  
source에서 target으로 motion transfer하기 위해 input은 source에서 pose stric figures를 추출하고  source에서 같은 포즈지만 target subject 이미지를 생성함 

#### Method

![img2](\assets\post\post6\img2.png)  
Input :  source person video, 다른 target person video  
Output: source motion처럼 행동하는 target의 새로운 video

제안하는 모델은 3 파트로 이루어져 있음  

1. pose detection  
   pre-trained state-of-the-art(openpose)를 활용하여 pose estimation
2. global pose normalization  
   source와 target body shape과 위치가 다른 부분을 다룸
3. normalized pose에서 target subject로 mapping   
   adversarial training을 사용하여 pose figure에서 target img에 mapping하는 것을 학습 

**Pose Encoding and Normalization**

* Encodoing body pose  
  ![img3](\assets\post\post6\img3.png)  
  pre-trained pose detector P(OpenPose model)를 사용함 
  이를 활용하여 color pose stick figure를 생성
* Global pose normalization  
  두 subject간 retargeting을 할 때 soure pose를 transfer  
  transfer에서 target의 body shape과 appearance를 일치시켜야함  
  subject의 pose에 대해 height와 pose위치를 사용하여 두 비디오에서 가장 가까운 발목위치와 가장 먼 발목 위치 간의  linear mapping을 함  
  position을 구하고 각 frame마다 scale과 translation을 계산함

**Pose to Video Translation**

pix2pix HD모델을 기반으로 사용하는데 목적에 맞게 수정함  
G는 pose stick figure가 주어지면 사람의 합성 이미지를 생성함  
기존의 signle-frame image-to-image translation 방법들은 video에 사용하면 temporal artifact를 생성하고 human motion에 대해서 detail을 생성할 수 없음  
그래서 본 논문에서는 temporal coherence 부분과 high resoluti[on 얼굴을 생성하는 부분을 추가하였음  

* Temporal smoothing  
  
  ![img5](\assets\post\post6\img5.png)  
  근접한 frame들 간의 temporal coherence들을 강화하기 위해서 Generator는 두 개의 연속적인 프레임들을 예측함   
  그래서 Discriminator는 image를 Fake or real과 구분 그리고 temporal coherance를 구분  
  ![img4](\assets\post\post6\img4.png)  
  두 개의 frame에 대해서 loss를 적용함  
  
* Face GAN  
  ![img6](\assets\post\post6\img6.png)  
  얼굴부분에 detail을 위해서 사용하는 part  
  nose 중심의 128x128부분이 input  
  output은 residual r ![img7](\assets\post\post6\img7.png)  
  마지막 합성 얼굴 영역은  main generator의 face region에다가 residual을 더함 
  r + G(x)  
  discriminator D_f는 "real" face pairs(x_F, y_F)인지 "fake" face paris(x_F, r+G(x)_F)인지 알아냄 (pix2pix와 비슷하게 loss를 설정)  
  ![img8](\assets\post\post6\img8.png)   
  full image와 유사하게 최종 얼굴 r+G(X)__F를 GT와 비교할 때 perceptrual reconstruction loss를 더함 
  
* Full Objective
  full image GAN은 face GAN과 분리되어 optimize 됨  
  ![img9](\assets\post\post6\img9.png)   
  temporal coherence + feature matching + perceptron reconstruction   
  ![img10](\assets\post\post6\img10.png) 은 pix2pixHD에서 discriminator feature-matching loss로 소개되었음 
  discriminator feature-matching loss는 discriminator 내부에서 activateion을 비슷하게 유도 새로운 목표를 지정하고 GAN의 불안정성을 해결 진짜와 가짜가 같은 feaature를 갖고 있는 확인 discriminator의 중간층의 activation func을 이용  
  preceptual reconstruction loss는 pretrained VGG feature와 network의 다른 layer의 feature를 비교하는 loss  
  ![img11](\assets\post\post6\img11.png)  
  full image GAN weight는 얼려지고 face를 optimizer함 위에 loss는 face GAN의 objective func

**Experiments**

* setup  
  dataset은 직접 촬영한 data, open source, single dancer target video
  * BaseLine method
    1. Nearest Neighbor  
       각 source video frame에서 pose distacne metric을 활용하여  학습된  target sequence와 가장 가깝게 match되는 frame을 찾음  
       p와p'이 각 n개의 joint에 대해서 p1,...,p_n 을 갖고 있고, 각 해당하는 joints에 대해서  L2 distance distance의 normalized sum을 통하여 distance를 정의함  
       ![img12](\assets\post\post6\img12.png)  
       인접한 target 일치 frame은 frame-by-frame의 가장 가까운 neighbor sequence로 concat 
    2. PoseWarp  
       target subject가 주어지면 새로운 pose 생성 하지만 이미지만 생성이 가능(제안하는 논문은 video)
  * Ablation Conditions  
    1. Frame-by-Frame : temporal smmothing을 평가
    2. Temporal smoothing(FBF+TS) : Face GAN을 평가
    3. Our model (FBF+TS+FG)
  * Evaluation metrics  
    1. perceptual studies on Mechanical Turk
    2. SSIM : Structural Similarity
    3. LPIPS : Learned Perceptual Image Patch Similarity   
       인간의 시각 지각 판단을 포함하여 patch로 유사성을 평가
    4. pose distance : input과 synthesized pose사이의 distacne를 평가 noise detection때문ㅇ에 유익하지 않음  
* Quantitative Evaluation  
  * Comparison to Baseline  
    더 realistic 한 것을 평가함 18 Pairs의 비디오 시간제한 없이 MTurk 100명에게 선택  
    ![img13](\assets\post\post6\img13.png)  
    NN, PoseWarp에 비해 이만큼 논문에 제안한 방법이 더 realistic 
* Ablation Study  
  ![img14](\assets\post\post6\img14.png)  
* Qualitative Result  
  ![img15](\assets\post\post6\img15.png)   
  ![img16](\assets\post\post6\img16.png)  

* Conclusion  
  제안하는 논문은 일반적인 source dancer의 움직임을 따라 춤추는 target의 길고 좋은 품질의 동영상을 만들 수 있음  
  하지만 그림에서 알 수 있듯이 옷이나 머리카락에 대해 실패 케이스가 있음  
  hight frequency texture를 사용하여 아티팩트를 완화함으로써 결과를 개선할 수 있었음  
  그리고 일반적인 포즈는 잘 따라할 수 있지만 물구나무서기나 학습데이터에 포함되지 않는 포즈에 대해서는 아티팩트가 발 생할 수 있음 



-> 다양한 어플리케이션에 적용할 수 있는 논문같아서 재밌었다 