---
layout: post
title:  "Everybody Dance Now"
date:   2021-01-26 16:012:36 +0530
categories: paper
---

맥북 주문한게 왔는데 불량이다 ㅠ 🤬

![img1](\assets\post\post6\img1.png)  
사람이 춤추는 동영상에서 새로운 target video에 transfer하는 논문  
video-to-video tranlation 느낌으로 보면 될거 같음  
source에서 부터 pose를 추출하고 target을 생성하기 위해 pose-to-appearancce mapping을 학습함  
시간적으로 일관된 비디오 결과를 위해 두개의 연속된 프레임을 예측함  
그리고 target video에 대해서 realistic을 위해 face 와 full image로 pipeline을 분리함  

#### Introduction

본 논문에서 simple하지만 효과적인 approach를 제안함 **"Do as I Do" video retargeting**  
두 개의 video가 주어지면 하나는 source 하나는 target  
source에서 target으로 motion을 transfer하는 논문 

frame-by-frame 방법으로 두 비디오 간의 motion을 transfer하기 위해서 두 개개인에 대해 이미지 간의 mapping을 학습해야함  
그래서 pix2pix(image-to-image translation)와 비슷하지만 본 논문은 pair한 이미지를 사용하지 않고, pair한 이미지가 있어도 두 object가 각 유니크한 body shape과 motion을 갖고 있어서 다름 

keypoint기반 포즈는 motion signatures를 유지하기 때문에 pose detector를 사용해서 pose를 추출함  
그리고 나서 pose stick figure이랑 target person 이미지들 간의 image-to-image를 translation model을 학습함  
source에서 target으로 motion transfer하기 위해 input은 source에서 pose stric figures를 추출하고  source에서 같은 포즈지만 target subject 이미지를 생성함 

#### Method

![img2](\assets\post\post6\img2.png)  
Input :  source person video, 다른 target person video  
Output: source motion처럼 행동하는 target의 새로운 video

제안하는 모델은 3 파트로 이루어져 있음  

1. pose detection  
   pre-trained state-of-the-art(openpose)를 활용하여 pose estimation
2. global pose normalization  
   source와 target body shape과 위치가 다른 부분을 다룸
3. normalized pose에서 target subject로 mapping   
   adversarial training을 사용하여 pose figure에서 target img에 mapping하는 것을 학습 

**Pose Encoding and Normalization**

* Encodoing body pose  
  ![img3](\assets\post\post6\img3.png)  
  pre-trained pose detector P(OpenPose model)를 사용함 
  이를 활용하여 color pose stick figure를 생성
* Global pose normalization  
  두 subject간 retargeting을 할 때 soure pose를 transfer  
  transfer에서 target의 body shape과 appearance를 일치시켜야함  
  subject의 pose에 대해 height와 pose위치를 사용하여 두 비디오에서 가장 가까운 발목위치와 가장 먼 발목 위치 간의  linear mapping을 함  
  position을 구하고 각 frame마다 scale과 translation을 계산함

**Pose to Video Translation**

pix2pix HD모델을 기반으로 사용하는데 목적에 맞게 수정함  
G는 pose stick figure가 주어지면 사람의 합성 이미지를 생성함  
기존의 signle-frame image-to-image translation 방법들은 video에 사용하면 temporal artifact를 생성하고 human motion에 대해서 detail을 생성할 수 없음  
그래서 본 논문에서는 temporal coherence 부분과 high resoluti[on 얼굴을 생성하는 부분을 추가하였음  

* Temporal smoothing  
  
  ![img5](\assets\post\post6\img5.png)  
  근접한 frame들 간의 temporal coherence들을 강화하기 위해서 Generator는 두 개의 연속적인 프레임들을 예측함   
  그래서 Discriminator는 image를 Fake or real과 구분 그리고 temporal coherance를 구분  
  ![img4](\assets\post\post6\img4.png)  
  두 개의 frame에 대해서 loss를 적용함  
  
* Face GAN  
  ![img6](\assets\post\post6\img6.png)  
  얼굴부분에 detail을 위해서 사용하는 part  
  nose 중심의 128x128부분이 input  
  output은 residual r ![img7](\assets\post\post6\img7.png)  
  마지막 합성 얼굴 영역은  main generator의 face region에다가 residual을 더함 
  r + G(x)  
  discriminator D_f는 "real" face pairs(x_F, y_F)인지 "fake" face paris(x_F, r+G(x)_F)인지 알아냄 (pix2pix와 비슷하게 loss를 설정)  
  ![img8](\assets\post\post6\img8.png)   
  full image와 유사하게 최종 얼굴 r+G(X)__F를 GT와 비교할 때 perceptrual reconstruction loss를 더함 
  
* Full Objective
  full image GAN은 face GAN과 분리되어 optimize 됨  
  ![img9](\assets\post\post6\img9.png)   
  temporal coherence + feature matching + perceptron reconstruction   
  ![img10](\assets\post\post6\img10.png) 은 pix2pixHD에서 discriminator feature-matching loss로 소개되었음 
  discriminator feature-matching loss는 discriminator 내부에서 activateion을 비슷하게 유도 새로운 목표를 지정하고 GAN의 불안정성을 해결 진짜와 가짜가 같은 feaature를 갖고 있는 확인 discriminator의 중간층의 activation func을 이용  
  preceptual reconstruction loss는 pretrained VGG feature와 network의 다른 layer의 feature를 비교하는 loss  
  ![img10](\assers\post\post6\img11.png)  
  full image GAN weight는 얼려지고 face를 optimizer함 위에 loss는 face GAN의 objective func

**Experiments**