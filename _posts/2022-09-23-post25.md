---
layout: post
title:  "SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation 정리"
date:   2022-09-24 10:32:36 +0530
categories: paper
---

정리는 넘 귀찮넹...🤡🤡



Semantic Segmentation에서 Transformer 구조보다 convolution 구조가 더 효율적임을 주장
 Semantic Segmentation key properties

* A strong Backbone
* Multi-scale information for objects of varying size
* Spatial attention 
* Low computational complexity 

we rethink the design of convolutional attention and propose an efficient yet effective encoder-decoder architecture 



1. Convolutional Encoder

   * hierarchical pyramid structure 사용 (4/8/16/32) 3x3 stride 2 conv - batch norm  
     (layer norm이 batch norm보다 성능이 좋음 in segmentation)

   * encoder는 ViT구조와 비슷하지만 self-attention mechanism을 사용하지 않고 novel multi-scale convolutional attention (MSCA) module을 제안함 

   * three parts

     - local information을 aggregate하는 depth-wise convolution 

     - multi-scale context를 포작하는 multi-branch depth-wise strip conv

     - 다양한 채널간 관계를 모델링하는 1x1 conv 
       ![img1](\assets\post\post25\img1.png)
       ![img2](\assets\post\post25\img2.png)
       $$\otimes$$ is the element-wise matrix multiplication operation
       $$i \in \{0,1,2,3\}$$ denotes the $$i$$ th branch $$Sacle_0$$ is the identity connection 

       each branch에서 depth-wise conv with large kernel (7, 11, 21)
       lightweight를 위해 7x1 1x7형식으로 mimic

2. Decoder
   three simple decoder structure 
   ![img2](\assets\post\post25\img3.png)

   * MLP-based structure

   * CNN-based models

   * SegNeXt
     last three stage로부터 features를 aggregate하고 lightweight Hamburger모델 사용

     



기존의 방법은 한번에 모든 오브젝트 카테고리에 대해 classifier를 진행함 하지만 이는 비슷한 카테고리에 대해 구별하기 어려움 
기존의 방법과는 다르게 1) confusing category에 대해 구별하기 위해서 general classifier + refinement classifier = integrated classifier를 사용함 2) confusing category에 대해 score가 근접하여 misclassification이 이루어질 수 있고 이에 대해 variance-based regularization을 제안함 

제안하는 방법은 다음과 같이 3단계로 이루어져있음

![img1](\assets\post\post24\img1.png)

**goal** 
$$\hat{c} = argmax_c P(O = o_c | I, r)$$

**1) feature extractor**
**2) general classification**

![img2](\assets\post\post24\img2.png)

multiple binary classifier  를 사용하는 이유는 위의 그림(오른쪽)과 같음 
일반적인 classifier는 비슷한 카테고리끼리라도 차이가 많이 남 
반대로 오른쪽 같은 binary의 경우는 차이가 별로 나지 않음 이는 multinomial 보다 상대적으로 높은 score를 제공함 그래서 binary classifier를 사용 

**3) refine classification**

multinomial classifier를 사용함 I와 general classifier의 prob을 input으로 사용 
general classifier에 높은 prob에 대해 category를 구분하고, general classifier의 오류 방지에 중점 


