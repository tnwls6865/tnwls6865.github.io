---
layout: post
title:  "An image is worth 16x16 words transformers for image recognition at scale"
date:   2021-01-25 16:012:36 +0530
categories: paper
---



NLP에서 핫하던 Transformer를 Vision 분야에 처음 적용하면서 엄청 🔥했던 논문



**Introduction**

![img1](\assets\post\post5\img1.png)

Attention is all you need 논문에서 transformer가 소개되면서 NLP분야의 다양한 task에 대해서 활용이 되어졌음    
Transformer  
RNN이나 LSTM없이 자체만으로 time sequence를 처리  
embedding -> embedding + position embedding(위치에 따른 다른 embedding) -> Encoder(new representation z) - Decoder(z로 부터 output sequence)구조

대형 Transformer기반 모델은 대형 corpus에 pre-trained한 후 GPT 및 BERT와 같은 다양한 task에 맞게 fine-tuning하고 이것은 NLP의 trend가 되었음  
Computer vision 분야에서 transforemr를 적용하려는 시도는 있었지만 효율적이지 않았고 대규모 이미지 인식에서 고전적인 ResNet이 base가 되는 모델들이 state of the art였음  
![img2](\assets\post\post5\img2.png)  
논문의 저자는 NLP 에서의 transformer확장의 성공에서 영감을 받아 가능한 최소한의 수정으로 이미지에 직접적으로 standard transformer를 적용하는 실험을 함  
그래서 supervision fashion으로 transformer를 활용하여 image classification에 적용함  
image를 patch로 바꾸어서 linear embedding의 squence로 사용(input) 그리고 NLP에서 word와 동일하게 transformer에서 사용함  
image에 transformer를 적용하는 것은 inductive bias가 부족할 수  있어 equivariance, locality와 같은 CNN 고유의 특징이 없어서 일반화가 잘되지 않아서 작은 데이터셋에 대해서는 실망스러운 결과를 보여주지만 larget dataset에 대해서는 inductive bais를 능가하는 결과를 보여줌 그래서 많은 데이터셋으로 학습 -> fine-tune(small dataset)이 좋음  
Inductive bias  
학습시 보지 못한 입력에 대한 출력을 예측할 때 사용된 가설  
학습자가 지금까지 만나보지 않았던 상황에서 정확한 예측을 하기 위해 사용하는 추가적인 가정(additional assumptions)을 의미함 

**Method**

image transfer는 NLP용으로 설계된 transformer구조를 따름 

* Reshape image  
  ![img4](\assets\post\post5\img4.png)  
  standard transformer은 token embedding의 1D sequence를 input으로 다룸  
  2D image를 처리하기 위해서 x를 flatted 2D patch sequence로 reshpe 해줘야함  
  (H,W)는 원본 이미지의 해상도 이고 P는 각 이미지 패치의 해상도이며 N은 transformer의 시퀀스 길이   
  그래서 HWC를 NP^2C로 reshape해줌 
* Patch embeddings  
  ![img5](\assets\post\post5\img5.png)  
  Transformer은 모든 계층에 대해 동일한 너비를 사용하기 땔문에 학습 가능한 linear projectio은 각 vectorized patch를 D차원에 매핑하고 그 결과를 Patch embedding이라고 함 
* Position embeddings  
  ![img6](\assets\post\post5\img6.png)   
  position 정보를 유지하기 위해 position embedding이 patch embedding에 추가됨  
  2-D aware posion embedding과 같은 다른 embedding에서 좋은 성능을 얻을 수 없어서 standard learnable 1D position embedding을 사용하였음   
  ![img7](\assets\post\post5\img7.png)   
  그리고 BERT의 [class] 토큰과 유사하게 학습 가능한 임베딩을 embedded patch의 sequece앞에 추가함(z0 = x_class)  
  BERT에서는 두 문장이 이어지는지 아닌지에 대한 token으로 앞에 class토큰을 사용함
* Transformer encoder  
  이러한 임베딩 벡터의 결과 sequence는 Encoder에 대한 입력으로 사용  
  ![img8](\assets\post\post5\img8.png)   
  Transforemr encoder는 multi-head self attention의 layer로 alternating하게 구성되어 있음  
  ![img9](\assets\post\post5\img9.png)   
  Layernorm(LN)은 모든 블록 이전에 적용되고 residual connection은 모든 블록 이후에 적용됨  
  GPT-1, BERT와 마찬가지로 relu보다 부드러운 형태인 GELU가 활성화 함수로 사용되었음  
  GELU는 음수에 대해서도 미분을 할 수 있으므로 약간 의 기울기를 제공할 수 있음  
  MSA : Multihead Self Attention  
  MLP : Multi-Layer Perceptron  
  GELU :  Gaussian Error Linear Unit activation function  
  MLP(Multi-Layer Perceptron)은 여러 층의 perceptron을 sequentially attachong하는 형태  
* 이게 전체적인 Vision Transformer모습으로 이미지를 패처로 변환 한 다음 class embedding 및 position정보를 추가하고 encoder에 대해 입력으로 사용하는 그림임  
  ![img10](\assets\post\post5\img10.png)   
* Hybrid Architecture  
  이미지 대신 Feature map을 사용하는 구조로 이미지 Projection을 ResNet의 초기 stage로 대체하여 실험한 모델 
* Fine Tuning And higher resolution  
  일번적으로 ViT는 대규모 데이터 세트에 대해 사전 학습되고 downstream task에 대해 fine-tuning을 함  
  이를 위해 pre-trained 된 prediction head를 제거하고 downstream task를 위한 layer를 붙여줌  
  pre-trained보다 높은 resoltuion에서 fine-tuning하는 것이 도움이 되는 경우가 많음  
  고해상도 이미지를 feeding할 때 patch크기가 동일하게 유지되기 때문에 sequnece 길이가 길어짐  
  따라서 pre-trained된 position embedding은 의미가 없을 수 있음  
  이에 대해 원본 이미지의 position에 맞게 pre-trained된 position embedding은 2D interpolation을 적용 

**Experiments**  

1. Dataset  
   실험에서 representation learning을 검증하고 다양한 크기의 dataset에 대한 pre-trained 및 benchmark를 사용하였음(실험은 주로 pre-trained된 후 fine-tuning으로 구성됨)  
   pre-trained dataset과 transfer learning dataset 2type의 데이터셋을 사용하였음  
   특히, JFT-300M는 구글에서 소개한 데이터셋으로 3억개 이상의 이미지를 가지고 있는 큰 데이터 셋임   

   * Pre-trained Dataset  
     ILSVRC-2012 ImageNet with 1k classes and 1.3M images  
     ImaegNet-21k with 21k classes and 14M images  
     JFT with 18k classes and 303M high-resolution  
   * Transfer learning Dataset  
     ImageNet on the original validation labels and cleaned-up ReaL labels  
     CIFAR-10/100  
     Oxford-IIIT Pets  
     Oxford Flowers-102  
     19-task VTAB classification suite (Natural/Specialized/Structured) -> 작업 당 1000개의 training예제를 사용해서 다양한 작업에 대해서 low-data transfer를 평가하는 dataset 

2. Model Variants  
   ![img11](\assets\post\post5\img11.png)   
   BERT기반 ViT모델 구성에 대한 요약  
   ViT-L/16는 Large variant이고 16x16 input patch size  

3. Training & Fine-tuning  
   ![img12](\assets\post\post5\img12.png) 

4. Comparison to stage of the art  
   ![img13](\assets\post\post5\img13.png)  
   모든 모델은 TPUv3에서 학습되었으며 pre-trained에 소요된 일수를 확인 할 수 있음 TPUv3코어(칩당 2개) * 일수  
   ViT-L/16 model은 BiT-L(large ResNet으로 학습시킨 모델)보다 모든 데이터셋에서 좋은 성능을 내지만 computational resource는 적게 필요함  
   larger model인 ViT-H/14 역시 성능은 향상시키지만 computational resource는 적게 듬   

   ![img14](\assets\post\post5\img14.png)  
   결과를 통해 제안된 viT가 잘 작동하는지 확인 할 수 있음  

   

  