---
layout: post
title:  "An image is worth 16x16 words transformers for image recognition at scale"
date:   2021-01-25 16:012:36 +0530
categories: paper
---



NLPì—ì„œ í•«í•˜ë˜ Transformerë¥¼ Vision ë¶„ì•¼ì— ì²˜ìŒ ì ìš©í•˜ë©´ì„œ ì—„ì²­ ğŸ”¥í–ˆë˜ ë…¼ë¬¸



**Introduction**

![img1](\assets\post\post5\img1.png)

Attention is all you need ë…¼ë¬¸ì—ì„œ transformerê°€ ì†Œê°œë˜ë©´ì„œ NLPë¶„ì•¼ì˜ ë‹¤ì–‘í•œ taskì— ëŒ€í•´ì„œ í™œìš©ì´ ë˜ì–´ì¡ŒìŒ    
Transformer  
RNNì´ë‚˜ LSTMì—†ì´ ìì²´ë§Œìœ¼ë¡œ time sequenceë¥¼ ì²˜ë¦¬  
embedding -> embedding + position embedding(ìœ„ì¹˜ì— ë”°ë¥¸ ë‹¤ë¥¸ embedding) -> Encoder(new representation z) - Decoder(zë¡œ ë¶€í„° output sequence)êµ¬ì¡°

ëŒ€í˜• Transformerê¸°ë°˜ ëª¨ë¸ì€ ëŒ€í˜• corpusì— pre-trainedí•œ í›„ GPT ë° BERTì™€ ê°™ì€ ë‹¤ì–‘í•œ taskì— ë§ê²Œ fine-tuningí•˜ê³  ì´ê²ƒì€ NLPì˜ trendê°€ ë˜ì—ˆìŒ  
Computer vision ë¶„ì•¼ì—ì„œ transforemrë¥¼ ì ìš©í•˜ë ¤ëŠ” ì‹œë„ëŠ” ìˆì—ˆì§€ë§Œ íš¨ìœ¨ì ì´ì§€ ì•Šì•˜ê³  ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ì¸ì‹ì—ì„œ ê³ ì „ì ì¸ ResNetì´ baseê°€ ë˜ëŠ” ëª¨ë¸ë“¤ì´ state of the artì˜€ìŒ  
![img2](\assets\post\post5\img2.png)  
ë…¼ë¬¸ì˜ ì €ìëŠ” NLP ì—ì„œì˜ transformerí™•ì¥ì˜ ì„±ê³µì—ì„œ ì˜ê°ì„ ë°›ì•„ ê°€ëŠ¥í•œ ìµœì†Œí•œì˜ ìˆ˜ì •ìœ¼ë¡œ ì´ë¯¸ì§€ì— ì§ì ‘ì ìœ¼ë¡œ standard transformerë¥¼ ì ìš©í•˜ëŠ” ì‹¤í—˜ì„ í•¨  
ê·¸ë˜ì„œ supervision fashionìœ¼ë¡œ transformerë¥¼ í™œìš©í•˜ì—¬ image classificationì— ì ìš©í•¨  
imageë¥¼ patchë¡œ ë°”ê¾¸ì–´ì„œ linear embeddingì˜ squenceë¡œ ì‚¬ìš©(input) ê·¸ë¦¬ê³  NLPì—ì„œ wordì™€ ë™ì¼í•˜ê²Œ transformerì—ì„œ ì‚¬ìš©í•¨  
imageì— transformerë¥¼ ì ìš©í•˜ëŠ” ê²ƒì€ inductive biasê°€ ë¶€ì¡±í•  ìˆ˜  ìˆì–´ equivariance, localityì™€ ê°™ì€ CNN ê³ ìœ ì˜ íŠ¹ì§•ì´ ì—†ì–´ì„œ ì¼ë°˜í™”ê°€ ì˜ë˜ì§€ ì•Šì•„ì„œ ì‘ì€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œëŠ” ì‹¤ë§ìŠ¤ëŸ¬ìš´ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ larget datasetì— ëŒ€í•´ì„œëŠ” inductive baisë¥¼ ëŠ¥ê°€í•˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ ê·¸ë˜ì„œ ë§ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ -> fine-tune(small dataset)ì´ ì¢‹ìŒ  
Inductive bias  
í•™ìŠµì‹œ ë³´ì§€ ëª»í•œ ì…ë ¥ì— ëŒ€í•œ ì¶œë ¥ì„ ì˜ˆì¸¡í•  ë•Œ ì‚¬ìš©ëœ ê°€ì„¤  
í•™ìŠµìê°€ ì§€ê¸ˆê¹Œì§€ ë§Œë‚˜ë³´ì§€ ì•Šì•˜ë˜ ìƒí™©ì—ì„œ ì •í™•í•œ ì˜ˆì¸¡ì„ í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ì¶”ê°€ì ì¸ ê°€ì •(additional assumptions)ì„ ì˜ë¯¸í•¨ 

**Method**

image transferëŠ” NLPìš©ìœ¼ë¡œ ì„¤ê³„ëœ transformerêµ¬ì¡°ë¥¼ ë”°ë¦„ 

* Reshape image  
  ![img4](\assets\post\post5\img4.png)  
  standard transformerì€ token embeddingì˜ 1D sequenceë¥¼ inputìœ¼ë¡œ ë‹¤ë£¸  
  2D imageë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œ xë¥¼ flatted 2D patch sequenceë¡œ reshpe í•´ì¤˜ì•¼í•¨  
  (H,W)ëŠ” ì›ë³¸ ì´ë¯¸ì§€ì˜ í•´ìƒë„ ì´ê³  PëŠ” ê° ì´ë¯¸ì§€ íŒ¨ì¹˜ì˜ í•´ìƒë„ì´ë©° Nì€ transformerì˜ ì‹œí€€ìŠ¤ ê¸¸ì´   
  ê·¸ë˜ì„œ HWCë¥¼ NP^2Cë¡œ reshapeí•´ì¤Œ 
* Patch embeddings  
  ![img5](\assets\post\post5\img5.png)  
  Transformerì€ ëª¨ë“  ê³„ì¸µì— ëŒ€í•´ ë™ì¼í•œ ë„ˆë¹„ë¥¼ ì‚¬ìš©í•˜ê¸° ë•”ë¬¸ì— í•™ìŠµ ê°€ëŠ¥í•œ linear projectioì€ ê° vectorized patchë¥¼ Dì°¨ì›ì— ë§¤í•‘í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ Patch embeddingì´ë¼ê³  í•¨ 
* Position embeddings  
  ![img6](\assets\post\post5\img6.png)   
  position ì •ë³´ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ position embeddingì´ patch embeddingì— ì¶”ê°€ë¨  
  2-D aware posion embeddingê³¼ ê°™ì€ ë‹¤ë¥¸ embeddingì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ì—†ì–´ì„œ standard learnable 1D position embeddingì„ ì‚¬ìš©í•˜ì˜€ìŒ   
  ![img7](\assets\post\post5\img7.png)   
  ê·¸ë¦¬ê³  BERTì˜ [class] í† í°ê³¼ ìœ ì‚¬í•˜ê²Œ í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”©ì„ embedded patchì˜ sequeceì•ì— ì¶”ê°€í•¨(z0 = x_class)  
  BERTì—ì„œëŠ” ë‘ ë¬¸ì¥ì´ ì´ì–´ì§€ëŠ”ì§€ ì•„ë‹Œì§€ì— ëŒ€í•œ tokenìœ¼ë¡œ ì•ì— classí† í°ì„ ì‚¬ìš©í•¨
* Transformer encoder  
  ì´ëŸ¬í•œ ì„ë² ë”© ë²¡í„°ì˜ ê²°ê³¼ sequenceëŠ” Encoderì— ëŒ€í•œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©  
  ![img8](\assets\post\post5\img8.png)   
  Transforemr encoderëŠ” multi-head self attentionì˜ layerë¡œ alternatingí•˜ê²Œ êµ¬ì„±ë˜ì–´ ìˆìŒ  
  ![img9](\assets\post\post5\img9.png)   
  Layernorm(LN)ì€ ëª¨ë“  ë¸”ë¡ ì´ì „ì— ì ìš©ë˜ê³  residual connectionì€ ëª¨ë“  ë¸”ë¡ ì´í›„ì— ì ìš©ë¨  
  GPT-1, BERTì™€ ë§ˆì°¬ê°€ì§€ë¡œ reluë³´ë‹¤ ë¶€ë“œëŸ¬ìš´ í˜•íƒœì¸ GELUê°€ í™œì„±í™” í•¨ìˆ˜ë¡œ ì‚¬ìš©ë˜ì—ˆìŒ  
  GELUëŠ” ìŒìˆ˜ì— ëŒ€í•´ì„œë„ ë¯¸ë¶„ì„ í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•½ê°„ ì˜ ê¸°ìš¸ê¸°ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŒ  
  MSA : Multihead Self Attention  
  MLP : Multi-Layer Perceptron  
  GELU :  Gaussian Error Linear Unit activation function  
  MLP(Multi-Layer Perceptron)ì€ ì—¬ëŸ¬ ì¸µì˜ perceptronì„ sequentially attachongí•˜ëŠ” í˜•íƒœ  
* ì´ê²Œ ì „ì²´ì ì¸ Vision Transformerëª¨ìŠµìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ íŒ¨ì²˜ë¡œ ë³€í™˜ í•œ ë‹¤ìŒ class embedding ë° positionì •ë³´ë¥¼ ì¶”ê°€í•˜ê³  encoderì— ëŒ€í•´ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê·¸ë¦¼ì„  
  ![img10](\assets\post\post5\img10.png)   
* Hybrid Architecture  
  ì´ë¯¸ì§€ ëŒ€ì‹  Feature mapì„ ì‚¬ìš©í•˜ëŠ” êµ¬ì¡°ë¡œ ì´ë¯¸ì§€ Projectionì„ ResNetì˜ ì´ˆê¸° stageë¡œ ëŒ€ì²´í•˜ì—¬ ì‹¤í—˜í•œ ëª¨ë¸ 
* Fine Tuning And higher resolution  
  ì¼ë²ˆì ìœ¼ë¡œ ViTëŠ” ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ ì‚¬ì „ í•™ìŠµë˜ê³  downstream taskì— ëŒ€í•´ fine-tuningì„ í•¨  
  ì´ë¥¼ ìœ„í•´ pre-trained ëœ prediction headë¥¼ ì œê±°í•˜ê³  downstream taskë¥¼ ìœ„í•œ layerë¥¼ ë¶™ì—¬ì¤Œ  
  pre-trainedë³´ë‹¤ ë†’ì€ resoltuionì—ì„œ fine-tuningí•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ë˜ëŠ” ê²½ìš°ê°€ ë§ìŒ  
  ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ feedingí•  ë•Œ patchí¬ê¸°ê°€ ë™ì¼í•˜ê²Œ ìœ ì§€ë˜ê¸° ë•Œë¬¸ì— sequnece ê¸¸ì´ê°€ ê¸¸ì–´ì§  
  ë”°ë¼ì„œ pre-trainedëœ position embeddingì€ ì˜ë¯¸ê°€ ì—†ì„ ìˆ˜ ìˆìŒ  
  ì´ì— ëŒ€í•´ ì›ë³¸ ì´ë¯¸ì§€ì˜ positionì— ë§ê²Œ pre-trainedëœ position embeddingì€ 2D interpolationì„ ì ìš© 

**Experiments**  

1. Dataset  
   ì‹¤í—˜ì—ì„œ representation learningì„ ê²€ì¦í•˜ê³  ë‹¤ì–‘í•œ í¬ê¸°ì˜ datasetì— ëŒ€í•œ pre-trained ë° benchmarkë¥¼ ì‚¬ìš©í•˜ì˜€ìŒ(ì‹¤í—˜ì€ ì£¼ë¡œ pre-trainedëœ í›„ fine-tuningìœ¼ë¡œ êµ¬ì„±ë¨)  
   pre-trained datasetê³¼ transfer learning dataset 2typeì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì˜€ìŒ  
   íŠ¹íˆ, JFT-300MëŠ” êµ¬ê¸€ì—ì„œ ì†Œê°œí•œ ë°ì´í„°ì…‹ìœ¼ë¡œ 3ì–µê°œ ì´ìƒì˜ ì´ë¯¸ì§€ë¥¼ ê°€ì§€ê³  ìˆëŠ” í° ë°ì´í„° ì…‹ì„   

   * Pre-trained Dataset  
     ILSVRC-2012 ImageNet with 1k classes and 1.3M images  
     ImaegNet-21k with 21k classes and 14M images  
     JFT with 18k classes and 303M high-resolution  
   * Transfer learning Dataset  
     ImageNet on the original validation labels and cleaned-up ReaL labels  
     CIFAR-10/100  
     Oxford-IIIT Pets  
     Oxford Flowers-102  
     19-task VTAB classification suite (Natural/Specialized/Structured) -> ì‘ì—… ë‹¹ 1000ê°œì˜ trainingì˜ˆì œë¥¼ ì‚¬ìš©í•´ì„œ ë‹¤ì–‘í•œ ì‘ì—…ì— ëŒ€í•´ì„œ low-data transferë¥¼ í‰ê°€í•˜ëŠ” dataset 

2. Model Variants  
   ![img11](\assets\post\post5\img11.png)   
   BERTê¸°ë°˜ ViTëª¨ë¸ êµ¬ì„±ì— ëŒ€í•œ ìš”ì•½  
   ViT-L/16ëŠ” Large variantì´ê³  16x16 input patch size  

3. Training & Fine-tuning  
   ![img12](\assets\post\post5\img12.png) 

4. Comparison to stage of the art  
   ![img13](\assets\post\post5\img13.png)  
   ëª¨ë“  ëª¨ë¸ì€ TPUv3ì—ì„œ í•™ìŠµë˜ì—ˆìœ¼ë©° pre-trainedì— ì†Œìš”ëœ ì¼ìˆ˜ë¥¼ í™•ì¸ í•  ìˆ˜ ìˆìŒ TPUv3ì½”ì–´(ì¹©ë‹¹ 2ê°œ) * ì¼ìˆ˜  
   ViT-L/16 modelì€ BiT-L(large ResNetìœ¼ë¡œ í•™ìŠµì‹œí‚¨ ëª¨ë¸)ë³´ë‹¤ ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì§€ë§Œ computational resourceëŠ” ì ê²Œ í•„ìš”í•¨  
   larger modelì¸ ViT-H/14 ì—­ì‹œ ì„±ëŠ¥ì€ í–¥ìƒì‹œí‚¤ì§€ë§Œ computational resourceëŠ” ì ê²Œ ë“¬   

   ![img14](\assets\post\post5\img14.png)  
   ê²°ê³¼ë¥¼ í†µí•´ ì œì•ˆëœ viTê°€ ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸ í•  ìˆ˜ ìˆìŒ  

   

  