---
layout: post
title:  "An image is worth 16x16 words transformers for image recognition at scale"
date:   2021-01-25 16:012:36 +0530
categories: paper
---



NLP에서 핫하던 Transformer를 Vision 분야에 처음 적용하면서 엄청 🔥했던 논문



**Introduction**

![img1](\assets\post\post5\img1.png)

Attention is all you need 논문에서 transformer가 소개되면서 NLP분야의 다양한 task에 대해서 활용이 되어졌음    
Transformer  
RNN이나 LSTM없이 자체만으로 time sequence를 처리  
embedding -> embedding + position embedding(위치에 따른 다른 embedding) -> Encoder(new representation z) - Decoder(z로 부터 output sequence)구조

대형 Transformer기반 모델은 대형 corpus에 pre-trained한 후 GPT 및 BERT와 같은 다양한 task에 맞게 fine-tuning하고 이것은 NLP의 trend가 되었음  
Computer vision 분야에서 transforemr를 적용하려는 시도는 있었지만 효율적이지 않았고 대규모 이미지 인식에서 고전적인 ResNet이 base가 되는 모델들이 state of the art였음  
![img2](\assets\post\post5\img2.png)  
논문의 저자는 NLP 에서의 transformer확장의 성공에서 영감을 받아 가능한 최소한의 수정으로 이미지에 직접적으로 standard transformer를 적용하는 실험을 함  
그래서 supervision fashion으로 transformer를 활용하여 image classification에 적용함  
image를 patch로 바꾸어서 linear embedding의 squence로 사용(input) 그리고 NLP에서 word와 동일하게 transformer에서 사용함  
image에 transformer를 적용하는 것은 inductive bias가 부족할 수  있어 equivariance, locality와 같은 CNN 고유의 특징이 없어서 일반화가 잘되지 않아서 작은 데이터셋에 대해서는 실망스러운 결과를 보여주지만 larget dataset에 대해서는 inductive bais를 능가하는 결과를 보여줌 그래서 많은 데이터셋으로 학습 -> fine-tune(small dataset)이 좋음  
Inductive bias  
학습시 보지 못한 입력에 대한 출력을 예측할 때 사용된 가설  
학습자가 지금까지 만나보지 않았던 상황에서 정확한 예측을 하기 위해 사용하는 추가적인 가정(additional assumptions)을 의미함 

**Method**

image transfer는 NLP용으로 설계된 transformer구조를 따름 

* Reshape image  
  ![img4](\assets\post\post5\img4.png)  
  standard transformer은 token embedding의 1D sequence를 input으로 다룸  
  2D image를 처리하기 위해서 x를 flatted 2D patch sequence로 reshpe 해줘야함  
  (H,W)는 원본 이미지의 해상도 이고 P는 각 이미지 패치의 해상도이며 N은 transformer의 시퀀스 길이   
  그래서 HWC를 NP^2C로 reshape해줌 
* Patch embeddings  
  ![img5](\assets\post\post5\img5.png)  
  Transformer은 모든 계층에 대해 동일한 너비를 사용하기 땔문에 학습 가능한 linear projectio은 각 vectorized patch를 D차원에 매핑하고 그 결과를 Patch embedding이라고 함 
* Position embeddings  
  ![img6](\assets\post\post5\img6.png)   
  position 정보를 유지하기 위해 position embedding이 patch embedding에 추가됨  
  2-D aware posion embedding과 같은 다른 embedding에서 좋은 성능을 얻을 수 없어서 standard learnable 1D position embedding을 사용하였음   
  ![img7](\assets\post\post5\img7.png)   
  그리고 BERT의 [class] 토큰과 유사하게 학습 가능한 임베딩을 embedded patch의 sequece앞에 추가함(z0 = x_class)  
  BERT에서는 두 문장이 이어지는지 아닌지에 대한 token으로 앞에 class토큰을 사용함
* Transformer encoder  
  이러한 임베딩 벡터의 결과 sequence는 Encoder에 대한 입력으로 사용  
  ![img8](\assets\post\post5\img8.png)   
  Transforemr encoder는 multi-head self attention의 layer로 alternating하게 구성되어 있음  
  ![img9](\assets\post\post5\img9.png)   
  Layernorm(LN)은 모든 블록 이전에 적용되고 residual connection은 모든 블록 이후에 적용됨  
  GPT-1, BERT와 마찬가지로 relu보다 부드러운 형태인 GELU가 활성화 함수로 사용되었음  
  GELU는 음수에 대해서도 미분을 할 수 있으므로 약간 의 기울기를 제공할 수 있음  
  MSA : Multihead Self Attention  
  MLP : Multi-Layer Perceptron  
  GELU :  Gaussian Error Linear Unit activation function  
  MLP(Multi-Layer Perceptron)은 여러 층의 perceptron을 sequentially attachong하는 형태  
* 이게 전체적인 Vision Transformer모습으로 이미지를 패처로 변환 한 다음 class embedding 및 position정보를 추가하고 encoder에 대해 입력으로 사용하는 그림임  
  ![img10](\assets\post\post5\img10.png)   
* Hybrid Architecture  
  이미지 대신 Feature map을 사용하는 구조로 이미지 Projection을 ResNet의 초기 stage로 대체하여 실험한 모델 
* Fine Tuning And higher resolution  
  일번적으로 ViT는 대규모 데이터 세트에 대해 사전 학습되고 downstream task에 대해 fine-tuning을 함  
  이를 위해 pre-trained 된 prediction head를 제거하고 downstream task를 위한 layer를 붙여줌  
  pre-trained보다 높은 resoltuion에서 fine-tuning하는 것이 도움이 되는 경우가 많음  
  고해상도 이미지를 feeding할 때 patch크기가 동일하게 유지되기 때문에 sequnece 길이가 길어짐  
  따라서 pre-trained된 position embedding은 의미가 없을 수 있음  
  이에 대해 원본 이미지의 position에 맞게 pre-trained된 position embedding은 2D interpolation을 적용 

**Experiments**  

1. Dataset  
   실험에서 representation learning을 검증하고 다양한 크기의 dataset에 대한 pre-trained 및 benchmark를 사용하였음(실험은 주로 pre-trained된 후 fine-tuning으로 구성됨)  
   pre-trained dataset과 transfer learning dataset 2type의 데이터셋을 사용하였음  
   특히, JFT-300M는 구글에서 소개한 데이터셋으로 3억개 이상의 이미지를 가지고 있는 큰 데이터 셋임   

   * Pre-trained Dataset  
     ILSVRC-2012 ImageNet with 1k classes and 1.3M images  
     ImaegNet-21k with 21k classes and 14M images  
     JFT with 18k classes and 303M high-resolution  
   * Transfer learning Dataset  
     ImageNet on the original validation labels and cleaned-up ReaL labels  
     CIFAR-10/100  
     Oxford-IIIT Pets  
     Oxford Flowers-102  
     19-task VTAB classification suite (Natural/Specialized/Structured) -> 작업 당 1000개의 training예제를 사용해서 다양한 작업에 대해서 low-data transfer를 평가하는 dataset 

2. Model Variants  
   ![img11](\assets\post\post5\img11.png)   
   BERT기반 ViT모델 구성에 대한 요약  
   ViT-L/16는 Large variant이고 16x16 input patch size  

3. Training & Fine-tuning  
   ![img12](\assets\post\post5\img12.png) 

4. Comparison to stage of the art  
   ![img13](\assets\post\post5\img13.png)  
   모든 모델은 TPUv3에서 학습되었으며 pre-trained에 소요된 일수를 확인 할 수 있음 TPUv3코어(칩당 2개) * 일수  
   ViT-L/16 model은 BiT-L(large ResNet으로 학습시킨 모델)보다 모든 데이터셋에서 좋은 성능을 내지만 computational resource는 적게 필요함  
   larger model인 ViT-H/14 역시 성능은 향상시키지만 computational resource는 적게 듬   

   ![img14](\assets\post\post5\img14.png)  
   결과를 통해 제안된 viT가 잘 작동하는지 확인 할 수 있음  

5. Pre-trained Data reqirements  
   inductive bias는 새로운 것을 정확하게 예측하기 위해 사용하는 추각의 가정임  
   적절한 inductive bias를 적용하는 의미는 주어진 데이터에 적절한 모델 아키텍쳐를 사용하는 것임  
   cnn은 이미지, sequence data는 rnn 또는 transformer  
   ViT는 이미지에 tranforemr를 적용하기 때문에 inductive bias가 없음  
   따라서 vision에 대한 inductive bias가 resnet보다 부족하지만 데이터셋의 크기가 얼마나 중요한지에 대한 실험을 진행함  
   ![img15](\assets\post\post5\img15.png)   
   가장 작은 데이터 셋에 대해 pre-trained를 수행할 때 ViT-Large모델은 정규화에도 불구하고 ViT-base보다 결과가 좋지 못함  
   그러나 imageNet-21k에서의 성능은 유사함  
   JFT-300M 데이터셋에서 pre-trained된 데\이터셋트의 크기가 증가할수록 ViT가 BiT보다 더 나은 성능을 보여줌  
   이 결과는 convolutional inductive bias는 작은 데이터세트에 대해서는 유용하지만 더 큰 데이터셋의 경우 관련 패턴을 학습하는 것만으로도 충분하고 유익하는것을 강조함  

6. Scaling Study  
   JFT-300M 데이터 셋에서 transfer performance에 대한 다양한 모델의 확장연구 실험  
   데이터셋 크기는 모델 성능에 병목 현상이 없으며 각 모델에 대한 성능 대 사전 학습 비용을 평가  
   ![img16](\assets\post\post5\img16.png)  
   결과를 통하여 ViT는 성능/계산 trade-off가 ResNet보다 효율적  
   ViT는 동일한 성능을 달성하기 위해 약 2배 적은 computing을 사용함  
   hybrid는 적은 계산 비용으로 ViT를 능가하지만 큰 계산 비용에서는 그 차이가 사라짐  
   또한 이를 통해 ViT는 시도 범위 내에서 saturate되지 않은 것으로 보이며 향후 확장 될 수 있음  

7. Inspective Vision Transformer  
   Vit가 이미지를 처리하는 방법을 이해하기 위해 분석하는 실험을 진행함  
   ![img17](\assets\post\post5\img17.png)  
   ViT의 첫 번째 layer는 flatten patch를 더 낮은 dimensional space로 projectiong함  
   왼쪽 그림은 학습한 embedgding filter의 주요 구성 요소를 visualization  
   구성 요소는 patch 내에서 microstructure의 저 차원의 representation을 위한 그럴듯한 basic funtion과 유사함 -> 기존의 cnn의 filter를 vis했을 때와 비슷하게 그럴듯하게 보인다라는 의미같음  
   projection 이후 position embedding이 patch representation에 추가 됨  
   가운데 그림은 모델이 position embedding의 similarity로 부터 이미지 내 거리를 encoding을 어떻게 하는지에 대해 보여줌 즉, 더 가까운 패치는 더 유사한 위치 임베딩을 갖는 경향이 있으며 행-열 구조가 나타남 -> 이해가 살짝 되지 않음  
   self-attention을 통하여 ViT는 가장 낮은 레이어데서도 전체 이미지에 대한 정보를 통합할 수 있음  
   오른쪽 그래프는 self-attention의 가중치를 기준으로 정보가 통합된 이미지 공간의 평균 거리를 계산한 것임  
   attention distance는 CNN의 receptive field size와 유사함  
   이 결과를 통해 일부 헤드는 이미 가장 낮은 레이어에 있는 대부분의 이미지에 대해 attention하고 네트워크 깊이에 따라 attention distance거리가 증가함

   ![img18](\assets\post\post5\img18.png) 
   attenti\on map result 를 통해 ViT가 사물을 잘 attention하는 것을 확인 할 수 있음  

**Conclusion**

논문에서는 이미지 인식에 transformer를 직접 적용하는 방법을 제안하였음  
Computer vision에서 self-attention을 사용하는 이전 연구와 달리 본 논문에서는 architecture에서 이미지 특정 inductive bias를 사용하지 않았음  
실험을 통해서 간단하미잔 확장 가능하고 전략적으로 잘 동작하는 것을 확인할 수 있었고 데규모 데이터셋에 대해 pre-trained과 결합될 때 역시 결과가 좋았음  
하지만 많은 과제를 남겨두고있음 다른 컴퓨터 비전 task에 대한 적용 더 좋은 성능 등과 같은 ... 





-> computer vision에 NLP에서 핫한 transformer을 적용했어서 매우 신기했지만 학습을 시키거나 테스트하기 위해선 많은 하드웨어가 필요하다는 점이 아쉬웠다.  요즘 모델들이 기업에서 나오다보니 아쉬운 점이 있다! 나의 연구는 언제쯤...😢 열심히하자 