---
layout: post
title:  "DeepGCNs Can GCNs Go as Deep as CNNs (정리 중)"
date:   2022-09-30 10:32:36 +0530
categories: paper
---

정리는 넘 귀찮넹...🤡🤡

요즘 날씨 넘 좋은거 아니냑우..😺😸



기존  graph convolution network는 gradient vanishing problem으로 인해 깊게 쌓을 수 없었음 back-propagation을 할때 over-smoothing 문제가 발생 

이러한 문제를 해결하고자 CNN에서 처럼 graph convolution network를 깊게 만드는 방법을 제안

**graph definition** 
graph(G) is a tuple (V, E)
V 는 unordered vertices , E는 vertices간의 connecting을 representation

graph convolution network
neighborhood로 부터 vertices의 특징을 aggregation하여 vertex에 대해 richer feature를 추출하는 것을 목표

each vertex v with feature vector $$h_v \in \mathbb{R}^D$$ , D는 feature dimension
graph g는 unordered vertices를 concat해 놓은 것과 같음 
$$h_g = [h_{v1}, h_{v2}, ..., h_{vg}] \in \mathbb{R}^{N \times D}$$ , N은 v의 cardinality 

일반적 graph convolution 
![img2](\assets\post\post26\img1.PNG)

$$W^{agg}, W^{update}$$ 는 learnable weights 
update, aggregate function은 다양하게 사용 
aggregate : mean, max-pooling, attention, LSTM 등등 
update : MLP, gate등등 

vertices의 representation은 neighbor vertices의 feature를 aggregating하여 각 layer에서 계산 되어짐 

![img2](\assets\post\post26\img2.PNG)

$$\theta$$ : update function, $$\rho$$ : aggregation function , $$N(v_l)$$ : neighbor vertices

