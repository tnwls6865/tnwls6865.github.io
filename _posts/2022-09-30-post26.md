---
layout: post
title:  "DeepGCNs Can GCNs Go as Deep as CNNs (ì •ë¦¬ ì¤‘)"
date:   2022-09-30 10:32:36 +0530
categories: paper
---

ì •ë¦¬ëŠ” ë„˜ ê·€ì°®ë„¹...ğŸ¤¡ğŸ¤¡

ìš”ì¦˜ ë‚ ì”¨ ë„˜ ì¢‹ì€ê±° ì•„ë‹ˆëƒ‘ìš°..ğŸ˜ºğŸ˜¸



ê¸°ì¡´  graph convolution networkëŠ” gradient vanishing problemìœ¼ë¡œ ì¸í•´ ê¹Šê²Œ ìŒ“ì„ ìˆ˜ ì—†ì—ˆìŒ back-propagationì„ í• ë•Œ over-smoothing ë¬¸ì œê°€ ë°œìƒ 

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì CNNì—ì„œ ì²˜ëŸ¼ graph convolution networkë¥¼ ê¹Šê²Œ ë§Œë“œëŠ” ë°©ë²•ì„ ì œì•ˆ

**graph definition** 
graph(G) is a tuple (V, E)
V ëŠ” unordered vertices , EëŠ” verticesê°„ì˜ connectingì„ representation

graph convolution network
neighborhoodë¡œ ë¶€í„° verticesì˜ íŠ¹ì§•ì„ aggregationí•˜ì—¬ vertexì— ëŒ€í•´ richer featureë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì„ ëª©í‘œ

each vertex v with feature vector $$h_v \in \mathbb{R}^D$$ , DëŠ” feature dimension
graph gëŠ” unordered verticesë¥¼ concatí•´ ë†“ì€ ê²ƒê³¼ ê°™ìŒ 
$$h_g = [h_{v1}, h_{v2}, ..., h_{vg}] \in \mathbb{R}^{N \times D}$$ , Nì€ vì˜ cardinality 

ì¼ë°˜ì  graph convolution 
![img2](\assets\post\post26\img1.PNG)

$$W^{agg}, W^{update}$$ ëŠ” learnable weights 
update, aggregate functionì€ ë‹¤ì–‘í•˜ê²Œ ì‚¬ìš© 
aggregate : mean, max-pooling, attention, LSTM ë“±ë“± 
update : MLP, gateë“±ë“± 

verticesì˜ representationì€ neighbor verticesì˜ featureë¥¼ aggregatingí•˜ì—¬ ê° layerì—ì„œ ê³„ì‚° ë˜ì–´ì§ 

![img2](\assets\post\post26\img2.PNG)

$$\theta$$ : update function, $$\rho$$ : aggregation function , $$N(v_l)$$ : neighbor vertices

