---
layout: post
title:  "Segmenting Transparent Object in the Wild with Transformer 정리"
date:   2021-03-15 20:34:36 +0530
categories: paper  
---



좀 더 열심히 😕😕

타겟을 transparent object로 잡아서 segmentation 

1. Fine-grained segmentation dataset

2. Trans2Seg
   CNN backbone, encoder-decoder, small conv head로 이루어져있음  
   ![img1](\assets\post\post12\img1.png)  

   * Feature map of CNN backbone (H/16, W/16, C)

   * Encoder
     Flattened feature of (H/16,W/16, C)   
     positional embedding of (H/16,W/16, C) : sequence에서 feature의 relative or absolute position에 대한 정보를 제공  
     The encoder is composed of stacked encoder layers, each of which consists of a multi-head self-attention module and a feed forward
     network

   * Decoder  
     ![img2](\assets\post\post12\img2.png)  
     learnable class prototype embedding : query (E_cls)  
     encoded feature : key and value (F_e)    
     ![img3](\assets\post\post12\img3.png)  
     output : new query   
     ![img4](\assets\post\post12\img4.png)   
     output : attention map(N, M, H/16, W/16)   
     N number of category, M number of multi-head attention

     upsampling (N, M H/4, W/4)

     fuse ResNet + attention map (N, M+C, H/4, W/4)

     final output (N, H/4, W/4)

