---
layout: post
title:  "Accelerating BERT Inference for Sequence Labeling via Early-Exit"
date:   2022-11-11 10:32:36 +0530
categories: paper

---

정리는 넘 귀찮넹...⚡⚡

아직 이전 graph  정리 다 안하고 재밌는 논문 발견



bert에 대해서 earlty-exit mechanism을 제안함   
inference stage에서 earler time에 만약 prediction가 confident하다면 전체 모델을 통과할 필요 없이 exit 하는 방법   
기존의 방법들은 sequence 전체에 대해서 confidence를 구하고 prediction을 하기 때문에 sequence-level eary exit하였음 하지만 이러한 방법은 token-level로 labeling이 필요한 sequence labeling task에는 적합하지 못함  
그래서 본 논문에서는 sequence labeling task에 맞는 early-exit 방법을 제안함   

* SENTence-level Early-Exit(SENTEE)  
  maximum uncertainty가 threshold보다 낮으면 early exit하는 방식 
* TOKen-lvel Early-Exit(TOKEE)  
  locality를 고려하여 token 자체가 아닌 window의 context를 기반으로 uncertainty를 구하고 이를 활용하여 early exit하는 방식 
* training중에 uncertainty에 기반해서 token을 멈추고 represnetation을 복사하는 fine-tuning방법을 제안 

text classification에서 early exit하는 방법 

![img1](\assets\post\post27\img1.PNG)

sequence labeling task에 대해 제안하는 sentence level and token level early exit하는 방법

![img2](\assets\post\post27\img2.PNG)

![img3](\assets\post\post27\img3.PNG)

**Token-Level Off-Ramps**    
(off-ramps: 이전 hidden state값을 다 합친 거)

PTM의 각 레이어를 linear classifier로 token-level과 결합
off-ramps가 golden label로 학습되면 instance는 전체 모델을 통과하는 대신 early exit할 수 있음   
![img4](\assets\post\post27\img4.PNG)

f는 l번째 레이어에서 token-levle off-ramps, P는 각 토큰에 대해 l번째 off-ramps에서 predicted label 분포

**Uncertainty of the Off-Ramp**  

![img5](\assets\post\post27\img5.PNG)  
다음과 같이 정보량(entropy)를 기반으로 uncertatinty를 계산함 (각 확률 분포에서의 정보량과 모든 cls의 발생확률이 같을 때의 정보량의 비?)

**Early-Exit Strategies**  

- SENTEE  
  token의 sequnce가 uncertainty가 충분히 낮으면 exit  
  전체 sequence에 대한 전체 uncertainty를 얻기 위해 각 token에 대해 uncertainty를 aggregate   
  ![img6](\assets\post\post27\img6.PNG)    
  u^l은 전체 sentence에 대한 uncertainty를 represent 
  미리 정해놓은 threshold보다 낮으면 exit   

  = 모델이 가장 어려운 토큰에 대한 예측을 확신하면 전체 시퀀스가 종료될 수 있음 

- TOKEE  
  sentence단위로하면 어려운 token이 여전히 uncertainty가 높을 수 있음 그래서 accelrate하기 위해서 token단위의 early exit방법을 제안   
  token의 label은 주변의 token에 크게 의존함 이에 대해 주어진 토큰 뿐만 아니라 그 맥락을 기반으로 uncertainty를 계산해야하기 때문에 window를 사용함   
  ![img7](\assets\post\post27\img7.PNG)  
  k는 미리 정해놓은 window size   

**Halt-and-Copy**

exit된 token은 upper layer에서 representation이 업데이트 되지 않고 복사됨   

* Halt : token의 uncertainty가 작다면 다음 layer에서도 예측이 변경되어질 가능성이 작다 
* Copy: token의 represnetation은 높은 confidence를 가진 label로 분류된다면 token의 representation은 이미 label information을 포함한다 따라서 token의  label을 예측하는데 도움이 되도록 upper layer에 그 representation을 복사함 

이러한 방법은 훈련중에 인접하지 않은 이전 layer로부터의 representation은 처리 하지 않아서 inference와 training간의 불일치가 발생할 수 있음   

**Model Training**

* Fine-Tuning for SENTEE  
  기존의 text classification에서 사용하는 jointly off-ramps를 더하는 방식을 사용  
   ![img8](\assets\post\post27\img8.PNG)  
  H는 cross entropy function 이에 대한 total loss는 weighted sum   
  ![img9](\assets\post\post27\img9.PNG)    
  w_l는 off-ramp에 대한 weight
* Fine-Tuning for TOKEE  
  training단계에서는 halt-copy를 안하기때문에 기존 joint training은 적합하지 않음 inference와 마찬가지로 이전 인접 layer뿐만 아니라 다른 이전 layer의 hidden representation을 사용하도록 모델을 학습하는 것이 목표   
  * Random Sampling  
    tokenl의 halting layer들은 uniform하게 샘플링   난이도에 따라 달라짐 
  * Self-Sampling  
    fine-tuned model자체를 사용하는거  
    각 epoch에 대해 window size와 threshold를 random으로 샘플링하고 해당 size와 th를 갖고 학습된 모델에 대해 TOKEE를 수행할 수 있음 우리는 각 token의 기존 layer를 얻고 re-forward할 때 사용함 이러한 방법은 token의 기존 layer은 난이도에 대응 할 수 있음 이렇게 함으로써 격차가 줄어들 수 있음



이후는 EXP 내용 
아직 이해가 안가는 부분은 

1. off-ramps를 찾아봤더니 이전 hidden representation을 합친 값이라고 하는데 그게 맞는가?
2. early exit방법은 training에 사용하지 않고 inference에서만 사용하는건가? 