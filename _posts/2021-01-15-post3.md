---
layout: post
title:  "End-to-End Object Detection with Transformers"
date:   2021-01-15 16:012:36 +0530
categories: paper
---



오랜만이지만..앞으로 더 꾸준히 읽길 바라며 🤡  

#### Abstract  

기존의 object detection방법을 사전지식을 이용하는 NMS(non-maximum suppression), anchor와 같은 것들을 지움으로써 모델 구조를 간소화하여 direct set prediction problem문제로 바꿔서 해결하려함  
제안하는 framework의 이름은 DEtection TRansformer or DETR이라고 부르고 bipartite matching과 transformer encoder-decoder구조를 활용하여 set-based global loss 를 활용함  
DETR은 image context와 object의 관계를 추출하고 구조가 매우 simple함 

#### Introduction  

현대 detector들은 indirect 방법으로 detection을 진행함  또한, object detection에서는 machine translation or speech recognition과 같은 end-to-end방식이 사용되지 않았었음  
그래서 논문에서는 direct set prediction problem으로 object detection의 pipline을 간소화하여 해결하려고 하며, transformer 기반의 encoder-decoder구조를 활용함  
![img1](/assets/post/post3/img1_.png)

Fig1은 DETR을 구조로 모든 object를 한번에 예측함 이때, predicted와 ground-truth object간에 bipartite matching을 통하여 set loss function활용하여 end-to-end로 학습할 수 있음 그리고 customized layer가 필요하지 않고 어떤 framework던지 쉽게 reproduced할 수 있음  
Direct set prediction작업을 하는 모델과 비교하였을 때, main 특징은 bipartite matching loss와 transformer를 사용하여 병렬적으로 decoding 한다는 것임 기존의 RNN의 경우 Autoregressive decoding에 집중하였지만 논문의 matching loss function은 유니크하게 GT object마다 하나의 prediction에 매칭되고 prediction object에 대해서 permuitation invariant해서 병렬적으로 사용할 수 있음  (permutation invariant란 입력 벡터 요소의 순서와 상관없이 같은 출력을 생성하는 모델)

#### The DETR model

* prediction 과 GT boxes 간의 유니크한 mathcing을 강하게 하는 a set prediction loss  

  1. Object detection set prediction loss  
     DETR은 고정된 N개의 prediction을 추론함 (내 생각에 N은 class..) 하나의 이미지에서 전형적으로 물체 갯수보다 더 많은 숫자로 N을 사용함  
     main difficulties는 GT에 맞춰 prediction objects(class, position, size)와의 score를 훈련시키는 것  
     그래서 제안하는 loss를 통해서 predicted와 GT object간의 optimal한 bipartite matching을 만들어냈고 이는 object-specific(bounding box) losses를 최적화 했음  
     ![loss1](/assets/post/post3/img2_.png)

     $y$ 는 ground truth, $\hat{y} = {\hat{y_i}^{N}_{i=1}}$ 는 N개의 prediction set N은 이미지 내의 객체 수 보다 많다고 가정하므로 $y$에 N size에 맞게 $\varnothing$ (no object)로 padding처리 하나의 요소가 N요소에 포함되는 것을 이분 매칭으로 찾았을 때  $\sigma$는 가장 낮음  
     즉, transformer의 디코더가 예측하는 객체의 class가 GT 객체 포함이 될 때, loss가 낮아짐  
     $\mathcal{L}_{match}(y_i, \hat{y_{\sigma(i)}})$는 GT와 prediction에 대한 pair-wise matching cost optimal assignment는 Hungarian algorithm을 활용함
     (TODO, Hungarian algorithm 설명 추가 하기 )  
     <img src="/assets/post/post3/img3_.png" alt="loss2" style="zoom:67%;" />

     $\mathcal{L}_{match}(y_i, \hat{y_{\sigma(i)}})$ 를 다음과 같이 표현할 수 있음 GT의 각 element $i$ 를 $y_i = (c_i, b_i)$라고 할 때, $c_i$는 class가 되고 $b_i$는 bbox가 됨 index $\sigma(i)$의 prediction에 대한 class $c_i$의 probabilitysms $\hat{p}_{\sigma(i)}(c_i)$(이때, 앞에 -를 붙이는 것은 probability는 확률이기 때문에 loss에 적용하기 위해서 -를 붙인 것 같음) 이고 prediction bbox는 $\hat{b}_{\sigma(i)}$임 이러한 과정은 one-to-one이므로 중복되는 prediction이 나오지 않고 Hungarian loss를 활용하여 모든 쌍을 매칭  
     <img src="/assets/post/post3/img4_.png" alt="loss3"/>

     이후 cross entropy 처럼 log를 적용하여 loss.  no object에 대해서 가중치에 10 감소를 줌 predction에 영향이 없어서 비용이 같음  
     Bounding box loss  
     기존 L1 loss는 같은 오류라도 bbox 사이즈에 따라서 error가 다름 그래서 scale에 불변하는 IoU loss를 활용  
     <img src="/assets/post/post3/img5_.png" alt="loss4" style="zoom:67%;" />

* Architecture 
  ![img6](/assets/post/post3/img6.png)
  세 가지 main components로 이루어져있음  

  1. CNN backbone(ResNet을 사용함)  
     기존의 CNN backbone모델로 lower-resolution activation map(32배로 줄임, C(채널)은 2048)을 생성함  
  2. encoder-decoder transformer  
     Encoder  
     1x1 conv를 사용해서 C(2048)를 작은 dimension d로 줄여줌  
     transformer encoder는 sequence가 input이기 때문에 feature map을 dxH/32xW/32 -> dxH/32*W/32  
     encoder layer는 기존의 standard구조이고 multi-head, self attention module, feed forward netwrok로 구성되어있음  
     transformer는 permutation-invariant이기 때문에, attention layer를 이용해  fixed positional encoding을 활용하였음  
     Decoder  
     decoder는 transformer의 표준 구조를 따르고, multi-head, self encoder-decoder attention mechanisms으로 d dimension을 N으로 embedding함  
     position embeeding과 비슷한 attion layer를 사용해서 object query를 추가함  
     object query인 N개의 prediction으로 decoder output을 얻음  
     학습에서 모델 outpute의 물체의 숫자가 정확할 수 있도록 Auxiliary decodinf loss로 FFN과 Hungarian loss를 각 decoder layer후에 추가함 
  3. simple feed forward network(FFN)
     마지막 prediction은  3-layer perceptron with RELU, hidden dimension d , linear projection layer로 얻음  
     output은 normalized된 center 좌표, Height, width 그리고  softmax를 사용할여 linear layer가 예측한 class label  

#### Experiments

Faster R-CNN과의 결과 비교  
![img7](/assets/post/post3/img7.png)

encoder의 크기에 따른 영향을 나타내는 결과  
encoder의 layer가 깊어질 수록 AP가 증가  
![img8](/assets/post/post3/img8.png)  

encoder의 self-attention visualization  
![img9](/assets/post/post3/img9.png)  

decoder layer에 따른 AP 결과  
![img10](/assets/post/post3/img10.png)

#### Conclusion

Object Detection에서 transformer기반 end-to-end방식의 새로운 구조를 제안함  
2 stage인 Faster R-CNN과 결과가 비슷하며(large 물체에선 더 좋은 결과) Panoptic segmentation으로 확장이 쉬움  





-> Direct로 물체를 matching시킨다는게 재밌었고, ViT이후 바로 object detection에 적용시켰다는게 흥미로웠음  
ViT논문도 곧 정리예정!

