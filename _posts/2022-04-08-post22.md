---
layout: post
title:  "CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning 정리"
date:   2022-04-08 10:32:36 +0530
categories: paper
---

요즘 날씨 넘 좋다,, 오늘도 논문 하나 읽어야지🌷🌷  
점점 정리가 늦어지고 있는건 비밀 



![img1](\assets\post\post22\img1.png)

기존 방법들은 two branch로 이루어져서 support branch는 query image의 segment를 guidance 하기 위해 feature를 extracting 함 

제안하는 방법도 마찬가지로 two branch design을 사용 

* dense comparison module 
  기존 pixel-level로 비교하는 방법들은 metric learning을 하려면 computational cost가 높았음. 제안하는 방법은 global average pooling을 통해 dense feature를 추출하고 computational cost도 줄인 상태에서 metric learning을 할 수 있음 

  * feature extract branch
    unseen classes의 특징을 추출하기 위해 high-level이 아닌 middle-level layer를 추출하여 학습
  * dense Comparison
    image에 target object와 background가 같이 있기 때문에, global average pooling을 통해 foreground feature를 squeeze함 
    query branch로부터 생성된 feature map에서의 spatial location과 support set의 foreground feature vector를 concat함(foreground영역만 추출하기 위해 binary support mask랑 element-wise multiplication)

* iterative optimization module 
  dense feature를 comparison하는 것은 충분하지 않아 iteratively refine하는 optimization module을 제안 
  residual connection structure를 사용하여 학습을 효율적으로 함 
  input : dense comparison을 통해 생성된 feature map, last iteration에서의 predict mask
  first forward pass에서는 prediction mask가 없어서 direct로 사용을 못함
  그래서 residual form을 활용하여 prediction mask를 활용
  $$M_t = x + F(x, y_{t-1})$$
  $$x$$ : output feature of the dense comparison module
  $$y_{t-1}$$ : prediction mask from the last iteration step
  $$M_t$$ : output of the residual block
  $$F()$$ : two 3x3 conv with 256 filter 를 통해  두 개의 feature 를 concat
  마지막에서는 Atrous Spatial Pyramid Pooling Moudle을 활용 
  dropout에서 영감을 받아 IOM의 input으로 alternatively last epoch의 prediction mask랑 empty mask를 사용

* attention module

  ![img2](\assets\post\post22\img2.png)  
  multiple support example로 부터 information을 활용하기 위해 attention module사용 
  two conv block 256 3x3 filter -> 3x3 max pooling -> 256 3x3 filter -> global average pooling 
  weight $$\lambda$$ 생성 
  $$\hat \lambda = {e^{\lambda_i} \over \sum_{j=1}^{k}e^{\lambda_j}}$$
  final output : 다양한 support sample에 의해 생성된 feature의 weighted sum 

