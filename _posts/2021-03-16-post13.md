---
layout: post
title:  "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"
date:   2021-03-16 20:34:36 +0530
categories: paper  
---



연구를 해야하는데 머리가 안돌아간다 🤕🤕

기존의 FCN 기반의 encoder-decoder구조가 아닌 sequence-to-sequce 형태로 transformer를 segmentation에 적용한 논문

**Introduction**

encoder-decoder : encoder는 feature representation 기반, decoder는 pixel-level classification을 함 그리고 이러한 design이 인기가 있는이유는 translation equivariance and locality때문임 하지만 segmentation에서 중요한 long range dependency information을 학습하는 것은 여전히 limited receptive fields로 어려워진다는 한계가 있음 

그래서 이러한 한계를 극복하고자 다양한 방법들이 연구됨   
conv operation을 조작하는 large kernel size와 atrous conv, image/feature pyramid등과 같은 방법들이 연구되었고 attention module into FCN architecture도 있음 FCN module은 바꾸지 않은채 attention과 결합하는 방법임 하지만 이또한 encoder-decoder 구조 encoder에서는 input resolution을 줄이고 decoder에서는 lower-resolution feature 를 개선하는 방법임 

그래서 본 논문에서는 encoder-decoder구조가 아닌 pure transformer구조를 적용한 SETR(SEgmentation TRansformer)를 제안함 