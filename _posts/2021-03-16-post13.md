---
layout: post
title:  "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"
date:   2021-03-16 20:34:36 +0530
categories: paper  
---



연구를 해야하는데 머리가 안돌아간다 🤕🤕

기존의 FCN 기반의 encoder-decoder구조가 아닌 sequence-to-sequce 형태로 transformer를 segmentation에 적용한 논문

**Introduction**

encoder-decoder : encoder는 feature representation 기반, decoder는 pixel-level classification을 함 그리고 이러한 design이 인기가 있는이유는 translation equivariance and locality때문임 하지만 segmentation에서 중요한 long range dependency information을 학습하는 것은 여전히 limited receptive fields로 어려워진다는 한계가 있음 

그래서 이러한 한계를 극복하고자 다양한 방법들이 연구됨   
conv operation을 조작하는 large kernel size와 atrous conv, image/feature pyramid등과 같은 방법들이 연구되었고 attention module into FCN architecture도 있음 FCN module은 바꾸지 않은채 attention과 결합하는 방법임 하지만 이또한 encoder-decoder 구조 encoder에서는 input resolution을 줄이고 decoder에서는 lower-resolution feature 를 개선하는 방법임 

그래서 본 논문에서는 encoder-decoder구조가 아닌 pure transformer구조를 적용한 SETR(SEgmentation TRansformer)를 제안함 

**Method**

FCN encoder는 conv layer가 sequentially하게 stack되어 있음 high layer에서 tensor 위치는 receptive field로 정의 되는 layer별 conv를 통해 모든 lower layer 의 tensor를 기반으로 계산되어짐 이러한 locality 때문에 layer의 depth에 따라 점점 receoptvie field를 높임 그 결과 big receptive field를 갖는 higher layer에서 long-range dependency를 모델링 할 수 있지만  더 많은 층을 추가할 때 특정 깊이에 도달하면 장점이 사라짐 따라서 vanilla FCN 구조 자체가 한계가 있음

그래서 FCN with attention mechanism이 제안되었음 하지만 이러한 방법은 연산량 때문에 higher layer에서만 attention을 적용할 수 있음 이는 low-level feature의 의존성이 부족하고 sub-optimal함 그래서 이러한 단점을 극복하기 위해서 SETR을 제안함

[img1](\assets\post\post13\img1.png)  

* Image to Sequence   
  transformer input 1D sequence of feature embedding  [img2](\assets\post\post13\img2.png)    
  flatten the image pixel은 연산량이 높음 
  그래서 transformer input sequence length L as [img3](\assets\post\post13\img3.png)    
  이후 reshape target feature map x_f  
  HW/256 - long input sequence를 얻기 위해 img (H x W x 3)를 grid path (H/16 x W /16)로 나누고 flatten   
  mapping each vectorized patch를 C-dimensinal embedding space를 얻기 위해 linear projection [img4](\assets\post\post13\img4.png)    이를 활용해서 1D sequence of patch embedding   
  To encode the patch spacial information, every location i에서 specific embedding p_i 를 학습함 그리고 마지막 final sequece input E를 형성하기 위해 p_i와 e_i를 더함 [img5](\assets\post\post13\img5.png)    이러한 방식으로 transformer의 self-attention 특성에도 불구하고 공간 정보가 유지됨 
* Transformer   
  1D embedding sequence E를 통해 feature representation -> global receptive field 을 사용해 기존 FCN encoder구조의 한계를 개선함  
  transformer encoder는 multi-head self-attention(MSA) and Multilayer Perceptron(MLP)의 L_e layer로 구성됨 

